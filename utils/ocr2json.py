import asyncio
import argparse
import json
import logging
import os
import sys
from pathlib import Path
import httpx

# flake8: noqa: E501


# ==============================================================================
# SCRIPT DE LIMPIEZA DE PREGUNTAS OCR CON LLM (VERSI√ìN 4)
#
# NUEVAS FUNCIONALIDADES:
# - El proveedor de IA es opcional, con 'openai' como valor por defecto.
# - Se a√±ade el argumento obligatorio `--anno` para especificar el a√±o del examen.
# - El campo `anno` se a√±ade a la estructura JSON final de cada pregunta.
#
# ------------------------------------------------------------------------------
# INSTRUCCIONES DE USO:
#
# 1. INSTALAR DEPENDENCIAS:
#    pip install httpx
#
# 2. CONFIGURAR VARIABLES DE ENTORNO:
#    - Para Gemini:
#      export GEMINI_API_KEY="TU_API_KEY_DE_GEMINI"
#    - Para OpenAI:
#      export OPENAI_API_KEY="TU_API_KEY_DE_OPENAI"
#
# 3. EJEMPLO DE EJECUCI√ìN:
#    python tu_script.py preguntas_sergas.txt --fuente "OPE SERGAS" --anno 2021
#    (Usar√° OpenAI por defecto)
#
#    python tu_script.py preguntas_mir.txt --fuente "Examen MIR" --anno 2023 --provider gemini
#    (Especificando el uso de Gemini)
# ==============================================================================


# --- Configuraci√≥n del Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)


async def call_gemini_api(client: httpx.AsyncClient, prompt_text: str, max_retries: int = 5) -> str | None:
    """Hace una llamada a la API de Gemini con reintentos y backoff exponencial."""
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        logging.error("La variable de entorno GEMINI_API_KEY no est√° configurada.")
        return None

    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}"

    payload = {
        "contents": [{"role": "user", "parts": [{"text": prompt_text}]}],
        "generationConfig": {
            "responseMimeType": "application/json",
            "temperature": 0.1,
            "maxOutputTokens": 4096,
        }
    }

    for attempt in range(max_retries):
        try:
            response = await client.post(api_url, json=payload, timeout=90.0)
            response.raise_for_status()

            result = response.json()
            if result.get("candidates"):
                return result["candidates"][0]["content"]["parts"][0]["text"]
            else:
                logging.warning(f"Respuesta OK de Gemini pero sin 'candidates'. Info: {response.text}")
                return None
        except httpx.HTTPStatusError as e:
            logging.error(
                f"Error de API Gemini (Intento {attempt + 1}/{max_retries}): Status {e.response.status_code}. Info: {e.response.text}")
        except httpx.RequestError as e:
            logging.error(f"Error de Red/Timeout (Intento {attempt + 1}/{max_retries}): {e}")

        if attempt < max_retries - 1:
            wait_time = 2 ** attempt
            logging.info(f"Reintentando en {wait_time} segundos...")
            await asyncio.sleep(wait_time)

    logging.error(f"Fallo al contactar la API de Gemini despu√©s de {max_retries} intentos.")
    return None


async def call_openai_api(client: httpx.AsyncClient, prompt_text: str, max_retries: int = 5) -> str | None:
    """Hace una llamada a la API de OpenAI con reintentos y backoff exponencial."""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logging.error("La variable de entorno OPENAI_API_KEY no est√° configurada.")
        return None

    api_url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Separamos el prompt en instrucciones de sistema y datos de usuario
    try:
        instructions, user_data = prompt_text.split("Ahora procesa el siguiente bloque:")
    except ValueError:
        logging.error("El formato del prompt es incorrecto. No se pudo dividir en instrucciones y datos.")
        # Usamos el prompt completo como contenido de usuario para intentar recuperarnos
        instructions = "Procesa el siguiente texto y devu√©lvelo en formato JSON."
        user_data = prompt_text

    payload = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role": "system", "content": instructions.strip()},
            {"role": "user", "content": user_data.strip()}
        ],
        "temperature": 0.1,
        "max_tokens": 2048,
        "response_format": {"type": "json_object"}
    }

    for attempt in range(max_retries):
        try:
            response = await client.post(api_url, json=payload, headers=headers, timeout=90.0)
            response.raise_for_status()

            result = response.json()
            if result.get("choices"):
                return result["choices"][0]["message"]["content"]
            else:
                logging.warning(f"Respuesta OK de OpenAI pero sin 'choices'. Info: {response.text}")
                return None
        except httpx.HTTPStatusError as e:
            logging.error(
                f"Error de API OpenAI (Intento {attempt + 1}/{max_retries}): Status {e.response.status_code}. Info: {e.response.text}")
        except httpx.RequestError as e:
            logging.error(f"Error de Red/Timeout (Intento {attempt + 1}/{max_retries}): {e}")

        if attempt < max_retries - 1:
            wait_time = 2 ** attempt
            logging.info(f"Reintentando en {wait_time} segundos...")
            await asyncio.sleep(wait_time)

    logging.error(f"Fallo al contactar la API de OpenAI despu√©s de {max_retries} intentos.")
    return None

# ==============================================================================
# L√ìGICA PRINCIPAL DEL SCRIPT
# ==============================================================================


def build_prompt(raw_question_block: str) -> str:
    """Construye el prompt detallado para enviar al LLM."""
    return f"""
Act√∫a como un asistente experto en la limpieza y estructuraci√≥n de datos para ex√°menes de oposici√≥n de sanidad en Espa√±a. Tu tarea es corregir los errores de OCR de un bloque de texto y formatearlo en una estructura JSON precisa.

**Instrucciones Clave:**
1.  **Analiza** el bloque de texto de entrada, que representa una √∫nica pregunta de opci√≥n m√∫ltiple.
2.  **Corrige** errores ortogr√°ficos, gramaticales y de formato. Usa tu conocimiento del dominio de la sanidad para interpretar abreviaturas y t√©rminos t√©cnicos.
3.  **Extrae la siguiente informaci√≥n**:
    - `numero`: El n√∫mero de la pregunta como un entero (integer).
    - `numero_original`: El texto del n√∫mero tal como aparece en el original (ej. "102.", "Pregunta: 99").
    - `enunciado`: El texto completo de la pregunta.
    - `opciones`: Un objeto con las claves "A", "B", "C", y "D".
4.  **Reconstruye** las opciones A, B, C y D, asegurando que cada una tenga su letra correcta, incluso si en el original faltan o est√°n mal.
5.  **Genera** una salida EXCLUSIVAMENTE en formato JSON v√°lido. No a√±adas explicaciones ni la palabra "json" antes o despu√©s del c√≥digo.
6.  **No inventes** informaci√≥n. Si algo es ilegible, m√°rcalo como `[ILEGIBLE]`.
7.  **Incluye** el texto original sin modificar en el campo `raw_ocr`.

**Ejemplo de la Tarea:**

---
**INPUT (Texto en bruto):**
```
102. Se le programa una cita a un paciente con varios estudios en el mismo dia. ¬øEn qu√© orden se realizar√≠an?
A)Rx simple de abdomen. esofagoygastroduodenal, urografia inlrivenosa.
B) Urograf√≠a intravenosa, rx simple de abdomen, esofacocastroduouenal.
C¬°Rx simple de abdomen, urograf√≠a intravenosa, esofagogastroduodenal,

D;) Ninguna de lis respuestas anteriores es correcta,
```

**OUTPUT (JSON Estricto):**
```json
{{
  "numero": 102,
  "numero_original": "102.",
  "enunciado": "Se le programa una cita a un paciente con varios estudios en el mismo d√≠a. ¬øEn qu√© orden se realizar√≠an?",
  "opciones": {{
    "A": "Rx simple de abdomen, esofagogastroduodenal, urograf√≠a intravenosa.",
    "B": "Urograf√≠a intravenosa, rx simple de abdomen, esofagogastroduodenal.",
    "C": "Rx simple de abdomen, urograf√≠a intravenosa, esofagogastroduodenal.",
    "D": "Ninguna de las respuestas anteriores es correcta."
  }},
  "raw_ocr": "102. Se le programa una cita a un paciente con varios estudios en el mismo dia. ¬øEn qu√© orden se realizar√≠an?\\nA)Rx simple de abdomen. esofagoygastroduodenal, urografia inlrivenosa.\\nB) Urograf√≠a intravenosa, rx simple de abdomen, esofacocastroduouenal.\\nC¬°Rx simple de abdomen, urograf√≠a intravenosa, esofagogastroduodenal,\\n\\nD;) Ninguna de lis respuestas anteriores es correcta,"
}}
```
---

Ahora procesa el siguiente bloque:

**INPUT:**
```
{raw_question_block}
```
"""


async def process_block(sem: asyncio.Semaphore, client: httpx.AsyncClient, block: str, provider: str) -> str | None:
    """Funci√≥n wrapper para procesar un bloque con control de concurrencia."""
    async with sem:
        prompt = build_prompt(block)
        if provider == 'gemini':
            return await call_gemini_api(client, prompt)
        elif provider == 'openai':
            return await call_openai_api(client, prompt)
        return None


async def main():
    """Punto de entrada principal del script."""
    parser = argparse.ArgumentParser(
        description="Limpia y estructura preguntas de un fichero OCR usando una API de LLM.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument("input_file", type=Path, help="Fichero de texto con las preguntas OCR en bruto.")
    parser.add_argument("--fuente", type=str, required=True,
                        help="Identificador y fuente del examen (ej. 'OPE SERGAS').")
    # <-- MODIFICACI√ìN 2: Se a√±ade el argumento obligatorio 'anno'
    parser.add_argument("--anno", type=int, required=True,
                        help="A√±o del examen (ej. 2021). Este valor se a√±adir√° a cada pregunta.")
    # <-- MODIFICACI√ìN 1: El argumento 'provider' ahora es opcional y tiene 'openai' por defecto
    parser.add_argument("--provider", type=str, choices=['gemini', 'openai'], default='openai',
                        help="Proveedor de LLM a utilizar (por defecto: 'openai').")
    parser.add_argument("-o", "--output_file", type=Path,
                        help="Fichero JSON de salida. Por defecto: [input_file_name].json")
    parser.add_argument("-s", "--separator", type=str, default="---",
                        help="Cadena separadora entre preguntas (por defecto: '---').")
    parser.add_argument("-c", "--concurrency", type=int, default=10,
                        help="N√∫mero de peticiones concurrentes a la API (por defecto: 10).")

    args = parser.parse_args()

    if not args.output_file:
        args.output_file = args.input_file.with_suffix(".json")

    # --- Lectura y preparaci√≥n de datos ---
    try:
        raw_text = args.input_file.read_text(encoding="utf-8")
        question_blocks = [block.strip() for block in raw_text.split(args.separator) if block.strip()]
        if not question_blocks:
            logging.warning(
                f"No se encontraron bloques de preguntas en '{args.input_file}' usando el separador '{args.separator}'.")
            sys.exit(0)
    except FileNotFoundError:
        logging.error(f"El fichero de entrada no existe: {args.input_file}")
        sys.exit(1)

    logging.info(
        f"üìÑ Encontrados {len(question_blocks)} bloques para procesar de la fuente '{args.fuente}' del a√±o {args.anno}.")

    # --- Bucle de procesamiento CONCURRENTE ---
    processed_questions = []
    semaphore = asyncio.Semaphore(args.concurrency)

    async with httpx.AsyncClient() as client:
        tasks = [process_block(semaphore, client, block, args.provider) for block in question_blocks]
        logging.info(
            f"‚öôÔ∏è  Enviando {len(tasks)} preguntas a la API '{args.provider}' con una concurrencia de {args.concurrency}...")
        api_responses = await asyncio.gather(*tasks, return_exceptions=True)

    logging.info("‚úÖ Todas las respuestas de la API han sido recibidas. Procediendo a parsear.")

    # --- Parseo y validaci√≥n de las respuestas ---
    for i, response in enumerate(api_responses):
        if isinstance(response, str) and response:
            try:
                clean_json_str = response.strip().removeprefix("```json").removesuffix("```").strip()

                if not clean_json_str:
                    logging.warning(f"La respuesta para el bloque {i + 1} estaba vac√≠a despu√©s de limpiar. Saltando.")
                    continue

                data = json.loads(clean_json_str)

                # A√±adimos los metadatos
                data["fuente"] = args.fuente
                # <-- MODIFICACI√ìN 2: Se a√±ade el a√±o a la estructura de datos
                data["anno"] = args.anno
                data["respuesta_correcta"] = None
                data["tags"] = []
                data["stats"] = {
                    "veces_preguntada": 0,
                    "veces_acertada": 0,
                    "veces_fallada": 0
                }

                processed_questions.append(data)
                logging.info(f"üëç Pregunta {data.get('numero', 'N/A')} (Bloque {i + 1}) procesada y estructurada.")

            except json.JSONDecodeError:
                logging.error(f"La respuesta para el bloque {i + 1} no es un JSON v√°lido.")
                logging.debug(f"Respuesta recibida (bloque {i + 1}):\n{response}")
            except Exception as e:
                logging.error(f"Error inesperado al procesar la respuesta del bloque {i + 1}: {e}")

        else:
            if isinstance(response, Exception):
                logging.error(f"Error en la tarea para el bloque {i + 1}: {response}")
            else:
                logging.warning(f"Respuesta inv√°lida o vac√≠a para el bloque {i + 1}. Saltando.")

    # --- Guardado del resultado final ---
    if not processed_questions:
        logging.warning("No se proces√≥ ninguna pregunta con √©xito. No se generar√° fichero de salida.")
        sys.exit(0)

    try:
        # Ordenamos las preguntas por su n√∫mero antes de guardarlas
        processed_questions.sort(key=lambda q: q.get('numero', 0))
        with open(args.output_file, "w", encoding="utf-8") as f:
            json.dump(processed_questions, f, ensure_ascii=False, indent=2)
        logging.info(f"\nüéâ ¬°Proceso completado! {len(processed_questions)} preguntas guardadas en: {args.output_file}")
    except Exception as e:
        logging.error(f"Error al guardar el fichero de salida: {e}")


if __name__ == "__main__":
    asyncio.run(main())
